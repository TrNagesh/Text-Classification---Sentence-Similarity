{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-03 04:38:41,575 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.19/tika-server-1.19.jar to /tmp/tika-server.jar.\n",
      "2019-03-03 04:38:57,631 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.19/tika-server-1.19.jar.md5 to /tmp/tika-server.jar.md5.\n",
      "2019-03-03 04:38:58,169 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2019-03-03 04:39:03,177 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background : \n",
      "\n",
      "In recent years, several annotation formats have been\n",
      "\n",
      "advanced to store and distribute biomedical information\n",
      "\n",
      "extraction outcomes. Commonly called annotations, they\n",
      "\n",
      "are generated following a specific structure or format de-\n",
      "\n",
      "pendent on the extraction system, and integration with ex-\n",
      "\n",
      "ternal databases and systems is challenging. IeXML (16)\n",
      "\n",
      "was one of the first XML-based implementations to define\n",
      "\n",
      "an exchange format considering annotations and enrich-\n",
      "\n",
      "ment of text. More recently, the BioC (17) has emerged as\n",
      "\n",
      "a community-supported format for encoding and sharing\n",
      "\n",
      "textual annotations. This simplified approach streamlines\n",
      "\n",
      "data reuse and sharing methods, achieving interoperability\n",
      "\n",
      "for the different text processing tasks by defining con-\n",
      "\n",
      "nectors to read and write XML annotations. Although\n",
      "\n",
      "they are created to enable interoperability and reusability\n",
      "\n",
      "between text-mined systems, these data structures are not\n",
      "\n",
      "designed to support data exploration and sustainability.\n",
      "\n",
      "To address this issue, it is necessary to develop new re-\n",
      "\n",
      "search methods to allow fast exploration and distribution\n",
      "\n",
      "of this valuable information.\n",
      "\n",
      "Emerging semantic web standards and concepts are\n",
      "\n",
      "playing an important role in solving data distribution prob-\n",
      "\n",
      "lems. In the scientific community, this is currently seen as\n",
      "\n",
      "the standard paradigm for data integration and distribu-\n",
      "\n",
      "tion on a web-scale, focused on the semantics and the con-\n",
      "\n",
      "text of data (18). It allows the construction of rich\n",
      "\n",
      "networks of linked data, offering advanced possibilities to\n",
      "\n",
      "retrieve and discover knowledge (e.g. reasoning). With the\n",
      "\n",
      "increasing adoption of this paradigm to tackle traditional\n",
      "\n",
      "data issues such as heterogeneity, distribution and inter-\n",
      "\n",
      "operability, novel knowledge-based databases and systems\n",
      "\n",
      "have been built to explore the potential behind this tech-\n",
      "\n",
      "nology. Essentially, they facilitate the deployment of\n",
      "\n",
      "well-structured data and deliver information in a usable\n",
      "\n",
      "structure for further analyses and reuse. In this way,\n",
      "\n",
      "approaches that combine the benefits of information ex-\n",
      "\n",
      "traction methods with these semantic systems represent a\n",
      "\n",
      "growing trend, allowing the establishment of curated data-\n",
      "\n",
      "bases with improved availability (19). Coulet et al. (20)\n",
      "\n",
      "provide an overview of such solutions, and describe a use\n",
      "\n",
      "case regarding the integration of heterogeneous text-mined\n",
      "\n",
      "pharmacogenomics relationships on the semantic web.\n",
      "\n",
      "Another case study is described by Mendes et al. (21), pre-\n",
      "\n",
      "senting a translation method for automated annotation of\n",
      "\n",
      "text documents to the DBpedia Knowledge Base (22). A\n",
      "\n",
      "different approach is proposed through the PubAnnotation\n",
      "\n",
      "(23) prototype repository. The notion was to construct a\n",
      "\n",
      "sharable store, where several corpora and annotations can\n",
      "\n",
      "be stored together and queried through SPARQL (24). In\n",
      "\n",
      "this perspective, there is a clear trend to combine text-\n",
      "\n",
      "mined information with semantic web technologies, result-\n",
      "\n",
      "ing in improved knowledge exchange and representation.\n",
      "\n",
      "Taking into account these approaches, there is a clear ten-\n",
      "\n",
      "dency towards workflow construction systems for annota-\n",
      "\n",
      "tion distribution. However, limitations in the development\n",
      "\n",
      "processes and the existence of software dependencies in the\n",
      "\n",
      "source platforms (25) represent a barrier to adapting and\n",
      "\n",
      "reusing existing solutions for the distribution of distinct an-\n",
      "\n",
      "notation structures and formats. The great heterogeneity of\n",
      "\n",
      "biomedical annotations makes it challenging to aggregate\n",
      "\n",
      "results obtained from different tools and systems, with\n",
      "\n",
      "Page 2 of 10 Database, Vol. 2017, Article ID bax088\n",
      "\n",
      "http://bioinformatics.ua.pt/dmd/scaleus/\n",
      "http://bioinformatics.ua.pt/dmd/scaleus/\n",
      "\n",
      "\n",
      "innovative solutions being necessary for multiple annota-\n",
      "\n",
      "tionsâ€™ combination and distribution.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import parser from tika\n",
    "from tika import parser\n",
    " \n",
    "# get a sample pdf\n",
    "parsedPDF = parser.from_file(\"bax088.pdf\")\n",
    " \n",
    "# print parsed PDF data\n",
    "#print (parsedPDF)\n",
    "#print (parsedPDF.viewkeys())\n",
    "#print (parsedPDF[\"metadata\"])\n",
    "#print (parsedPDF[\"content\"])\n",
    "\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "def find_between_r( s, first, last ):\n",
    "    try:\n",
    "        start = s.rindex( first ) + len( first )\n",
    "        end = s.rindex( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "abstract = find_between(parsedPDF[\"content\"], \"Abstract\", \"Introduction\" )\n",
    "intro = find_between(parsedPDF[\"content\"], \"Introduction\", \"Background\" )\n",
    "background = find_between(parsedPDF[\"content\"], \"Background\", \"Materials\" )\n",
    "method = find_between(parsedPDF[\"content\"], \"Materials and methods\", \"Results\" )\n",
    "\n",
    "\n",
    "print(\"Background :\", background)\n",
    "\n",
    "#print (find_between(parsedPDF[\"content\"], \"Abstract\", \"Introduction\" ))\n",
    "#print (find_between_r( parsedPDF[\"content\"], \"Abstract\", \"Introduction\"))\n",
    "\n",
    "#p = re.compile(r'?:ABSTRACT\\s*\\n+|Abstract\\s*\\n+)(.*?)((?:[A-Z]+|(?:\\n(?:[A-Z]+|(?:[A-Z][a-z]+\\s*)+)\\n+)')\n",
    "\n",
    "#re.findall(p, parsedPDF[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Computational annotation of textual information has taken on an important role in knowledge\n",
      "\n",
      "extraction from the biomedical literature, since most of the relevant information from scien-\n",
      "\n",
      "tific findings is still maintained in text format.\n",
      "In this endeavour, annotation tools can assist in\n",
      "\n",
      "the identification of biomedical concepts and their relationships, providing faster reading and\n",
      "\n",
      "curation processes, with reduced costs.\n",
      "However, the separate usage of distinct annotation\n",
      "\n",
      "systems results in highly heterogeneous data, as it is difficult to efficiently combine and ex-\n",
      "\n",
      "change this valuable asset.\n",
      "Moreover, despite the existence of several annotation formats,\n",
      "\n",
      "there is no unified way to integrate miscellaneous annotation outcomes into a reusable, shar-\n",
      "\n",
      "able and searchable structure.\n",
      "Taking up this challenge, we present a modular architecture\n",
      "\n",
      "for textual information integration using semantic web features and services.\n",
      "The solution\n",
      "\n",
      "described allows the migration of curation data into a common model, providing a suitable\n",
      "\n",
      "transition process in which multiple annotation data can be integrated and enriched, with the\n",
      "\n",
      "possibility of being shared, compared and reused across semantic knowledge bases.\n"
     ]
    }
   ],
   "source": [
    "###tokenizing abstract#####\n",
    "### store the last sentence of abstract in abc1.txt file\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.data\n",
    "\n",
    "abstract_sent =sent_tokenize(abstract)\n",
    "#print(abstract_sent)\n",
    "\n",
    "for i,sent in enumerate(abstract_sent):\n",
    "    print(sent)\n",
    "    \n",
    "with open('abc1.txt', 'w') as f:\n",
    "    print(sent, file=f)\n",
    "    \n",
    "    \n",
    "#f = open('abc1.txt','w')\n",
    "#print(abstract_sent, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarizing the text ####\n",
    "### here I am summarizing introduction part - intro\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(intro)\n",
    "\n",
    "freqTable = dict()\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in stopWords:\n",
    "        continue\n",
    "\n",
    "    word = stemmer.stem(word)\n",
    "\n",
    "    if word in freqTable:\n",
    "        freqTable[word] += 1\n",
    "    else:\n",
    "        freqTable[word] = 1\n",
    "\n",
    "sentences = sent_tokenize(intro)\n",
    "sentenceValue = dict()\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word, freq in freqTable.items():\n",
    "        if word in sentence.lower():\n",
    "            if sentence in sentenceValue:\n",
    "                sentenceValue[sentence] += freq\n",
    "            else:\n",
    "                sentenceValue[sentence] = freq\n",
    "\n",
    "\n",
    "\n",
    "sumValues = 0\n",
    "for sentence in sentenceValue:\n",
    "    sumValues += sentenceValue[sentence]\n",
    "\n",
    "# Average value of a sentence from original text\n",
    "average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "\n",
    "summary = ''\n",
    "for sentence in sentences:\n",
    "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
    "        summary += \" \" + sentence\n",
    "\n",
    "#print(summary)\n",
    "\n",
    "with open('abc2.txt', 'a') as f1:\n",
    "    print(summary, file=f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we have last sentence of abstarct in abc1 and Summarized text from introduction in abc2\n",
    "## now let us see the similarity using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words(\"english\")\n",
    "def preprocessing(raw):\n",
    "    wordlist = nltk.word_tokenize(raw)\n",
    "    text = [w.lower() for w in wordlist if w not in stopwords_en]\n",
    "    return text\n",
    "                               \n",
    "f1 = open('abc1.txt','r',encoding ='utf-8')\n",
    "text1 = preprocessing(f1.read())\n",
    "\n",
    "                               \n",
    "f2 = open('abc2.txt','r',encoding ='utf-8')\n",
    "text2 = preprocessing(f2.read())\n",
    "\n",
    "#f3 = open('abc3.txt','r',encoding ='utf-8')\n",
    "#text3 = preprocessing(f3.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Index : 36.72 % \n"
     ]
    }
   ],
   "source": [
    "## Frequency count method\n",
    "from nltk.probability import FreqDist\n",
    "word_set = set(text1).union(set(text2))\n",
    "\n",
    "freqd_text1 = FreqDist(text1)\n",
    "text1_count_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_count_dict[word]= freqd_text1[word]\n",
    "\n",
    "freqd_text2 = FreqDist(text2)\n",
    "text2_count_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_count_dict[word]= freqd_text2[word]\n",
    "\n",
    "#text1_count_dict\n",
    "v1 = list(text1_count_dict.values())\n",
    "v2 = list(text2_count_dict.values())\n",
    "\n",
    "similarity = 1 - nltk.cluster.cosine_distance(v1,v2)\n",
    "print(\"Similarity Index : {:4.2f} % \".format(similarity*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF calculations\n",
    "freqd_text1 = FreqDist(text1)\n",
    "text1_length = len(text1)\n",
    "text1_tf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_tf_dict[word]= freqd_text1[word]/text1_length\n",
    "\n",
    "freqd_text2 = FreqDist(text2)\n",
    "text2_length = len(text2)\n",
    "text2_tf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_tf_dict[word]= freqd_text2[word]/text2_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IDF calculations\n",
    "\n",
    "text12_idf_dict = dict.fromkeys(word_set,0)\n",
    "text12_length = 2\n",
    "for word in text12_idf_dict.keys():\n",
    "    if word in text1:\n",
    "        text12_idf_dict[word] +=1\n",
    "    if word in text2:\n",
    "        text12_idf_dict[word] +=1\n",
    "        \n",
    "import math\n",
    "for word,val in text12_idf_dict.items():\n",
    "    text12_idf_dict[word] = 1 + math.log(text12_length/(float(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TDIF of a word\n",
    "\n",
    "text1_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_tfidf_dict[word] = (text1_tf_dict[word])*(text12_idf_dict[word])\n",
    "\n",
    "text2_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_tfidf_dict[word] = (text2_tf_dict[word])*(text12_idf_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Index : 18.14 % \n"
     ]
    }
   ],
   "source": [
    "## compute cosine similarity\n",
    "\n",
    "v1 = list(text1_tfidf_dict.values())\n",
    "v2 = list(text2_tfidf_dict.values())\n",
    "similarity = 1 - nltk.cluster.cosine_distance(v1,v2)\n",
    "print(\"Similarity Index : {:4.2f} % \".format(similarity*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "Training epoch 10\n",
      "Training epoch 20\n",
      "Training epoch 30\n",
      "Training epoch 40\n",
      "Training epoch 50\n",
      "Training epoch 60\n",
      "Training epoch 70\n"
     ]
    }
   ],
   "source": [
    "## doc2vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "taggeddocs=[]\n",
    "\n",
    "doc1 = TaggedDocument(words=text1,tags =[u'Abstract'])\n",
    "taggeddocs.append(doc1)\n",
    "#doc2 = TaggedDocument(words=text2,tags =[u'Intro'])\n",
    "#taggeddocs.append(doc2)\n",
    "\n",
    "\n",
    "doc3 = TaggedDocument(words=text2,tags =[u'Background'])\n",
    "taggeddocs.append(doc3)\n",
    "## build the model\n",
    "\n",
    "model = gensim.models.Doc2Vec(taggeddocs,dm=0,alpha = 0.025, size=10, min_alpha=0.025, min_count=0)\n",
    "\n",
    "#training\n",
    "\n",
    "\n",
    "for epoch in range(80):\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Training epoch %s\" % epoch)\n",
    "    model.train(taggeddocs,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "    model.alpha -=0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Index : 63.49 % \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "similarity = model.n_similarity(text1,text2)\n",
    "print(\"Similarity Index : {:4.2f} % \".format(similarity*100)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
